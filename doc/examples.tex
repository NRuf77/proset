\chapter{Fit strategy with examples}
\label{ch_examples}
%
\section{Fit strategy}
\label{sec_fit_strategy}
%
Fitting a proset model is controlled by seven hyperparameters:
%
\begin{itemize}
\item The number of batches $B$.
%
\item The number $M$ of candidates for prototypes evaluated per batch.
%
\item The maximum fraction $\eta$ of candidates drawn from one bin (see Algorithm \ref{alg_bins}).
%
\item The penalty term $\lambda_v$ for the feature weights.
%
\item The penalty term $\lambda_w$ for the prototype weights.
%
\item The ratio $\alpha_v$ of $\lambda_v$ assigned as $l_2$ penalty term.
%
\item The ratio $\alpha_w$ of $\lambda_w$ assigned as $l_2$ penalty term.
\end{itemize}
%
One goal of the case studies in this section is to identify good default values and indicate which parameters are be worthwhile to tune.
Some of the key findings are summarized here:
%
\begin{enumerate}
\item Fitting a few large batches is preferrable to many smaller ones.
The first batch has the largest impact on the model score and a large first batch results in a better overall score.
We believe the reason for this is that model quality depends on finding good constellations of prototypes, not just individual prototypes.
Suitable constellations are more likely to occur in larger batches.\par
%
Adding too many batches to a model leads to saturation, not overfitting.
The number of prototypes chosen per batch decreases, possibly down to zero.
The corresponding model scores fluctuate slightly around a common level.
Thus, using a small number of batches is mostly a question of reducing computational effort.\par
%
We recommend $M=1,000$ candidates per batch and a single batch ($B=1$) as default.
If optimizing with respect to the number of batches, $B=10$ is a reasonable upper limit.
%
\item If the number of samples in a bin is too small for drawing the desired number of candidates, a reasonable compromise is to use half of the samples as candidates and the others as reference points for computing the likelihood.
Thus, we fix $\eta=0.5$.
%
\item The penalty $\lambda_v$ for the feature weights controls the smoothness of the model.
It is the main safeguard against overfitting.\par
%
In the case study for the classifier, good values lie in the range from $10^{-6}$ to $10^{-1}$.
Choosing $10^{-3}$ as default works for all cases.
However, if an experimenter wants to tune only one parameter, it should be $\lambda_v$.
%
\item The penalty $\lambda_w$ for the prototype weights controls how much weight can be assigned to a single prototype.
We observe that increasing $\lambda_w$ within a certain range can actually lead to more candidate points being included in the model with non-zero weight.
Further increases gradually lead to underfitting.
However, reducing $\lambda_w$ does not lead to appreciable overfitting.
It appears that values below a certain threshold mostly control the preference for few prototypes with large weights versus more prototypes with smaller weights.
Above the threshold, the predicted distribution is shrunk towards the marginal probabilities.
This is desirable to some degree for avoiding overfitting.\par
%
In the case study for the classifier, good values lie in the range from $10^{-9}$ to $10^{-4}$.
Choosing $10^{-8}$ as default works for all cases.
%
\item If the $l_2$-penalty is dominant ($\alpha_v$ and $\alpha_w$ close to 1.0), the fitting procedure tends to select slightly more features and produce slightly better model scores than if the $l_1$-penalty is dominant ($\alpha_v$ and $\alpha_w$ close to 0.0).
While this can be seen as a matter of taste, we recommend giving a larger impact to the $l_2$-penalty.
This is because we have observed unstable behavior using a dominant $l_1$-penalty for small sample sizes.
If the model is re-fitted on the entire training data after cross-validation, the final model may underfit.
Too few features are included with a corresponding decrease in model quality.
The likely cause is the random candidate selection, which may result in less suitable constellations of points if sample size is small.\par
%
The default values used by the algorithm are $\alpha_v=\alpha_w=0.95$.
\end{enumerate}
%
The case study uses the following procedure for parameter search:
%
\begin{algorithm}[Hyperparameter selection]~
\label{alg_hyperparameters}
\begin{enumerate}
\item Choose $M$, $\eta$, $\alpha_v$, and $\alpha_w$ based on the above recommendations.
Choose a range for $\lambda_v$, a range for $\lambda_w$, and a set of candidates for $B$ (e.g., the numbers from 0 to 10).
%
\item Split the data into a training set (70 \%) and test set (30 \%), stratified by class in case of classification.
%
\item\textbf{Stage 1}
%
\begin{enumerate}
\item Randomly generate 50 pairs $(\lambda_v,\lambda_w)$.
Each $\lambda$ is sampled uniformly on the log-scale from the chosen range.
%
\item Perform five-fold cross-validation on the training set using $B=1$.
%
\item Compute the mean and standard deviation of log-loss per pair over the left-out folds.
%
\item Determine a threshold for model quality by taking the minimal mean log-loss and adding the corresponding standard deviation.
%
\item Among all pairs whose mean log-loss is less than or equal to the threshold, choose the one maximizing the geometric mean $\sqrt{\lambda_v\lambda_w}$.
\end{enumerate}
%
\item\textbf{Stage 2}
\begin{enumerate}
\item Perform five-fold cross-validation on the training set using the parameters from stage 1 and the maximal number of batches.
%
\item For each candidate value of $B$, evaluate the models on the left-out fold and compute mean and standard deviation of log-loss.
%
\item Determine a threshold for model quality by taking the minimal mean log-loss and adding the corresponding standard deviation.
%
\item Among all candidates whose mean log-loss is less than or equal to the threshold, choose the smallest $B$.
\end{enumerate}
%
\item Refit the model with parameters selected in stages 1 and 2 on all training data.
%
\item Score the final model on the test data.
\end{enumerate}
\end{algorithm}
%
The purpose of stage 1 is to identify good values for the most important parameters $\lambda_v$ and $\lambda_w$.
Based on the observation that the first batch has the most impact, we fix $B=1$ during this stage to save computation time.
Controlling $B$ can be treated as a secondary concern, since it appears to be impossible to overfit by increasing $B$.
It is still necessary to test larger values as more complex problems may be underfitted with $B=1$.\par
%
Note that stage 2 only fits five models up to the maximum number of batches, but these can be evaluated also for smaller values of $B$.
This introduces a dependency between the means and standard deviations as estimates reuse the same initial batches.
However, as keeping $B$ small is a secondary concern, we consider this time-saving approach acceptable.\par
%
The `one standard error rule' for replacing the optimal value found via cross-validation by a sparser solution of similar quality is a recommendation from R package \texttt{glmnet} \cite{Friedman_10}.
The authors observe that the maximizer from cross-validation tends to slightly overfit the cross-validation sample, which is mitigated by the rule.
%
\section{Classification}
\label{sec_classification}
%
In this section, we consider how to choose hyperparameters for the proset classifier and compare the results to those obtainable for other machine learning models.
%
\subsection{Fitting the proset classifier}
\label{sec_fit_classifier}
%
For the parameter study, we use four small data sets provided as `toy examples' by Python package \texttt{sklearn} \cite{Pedregosa_11}, plus two slightly larger artificial data sets:
%
\begin{enumerate}
\item\textbf{Iris 2f:} this data set consists of the first two features of Fisher's famous iris data set \cite{Fisher_36}.
We limit the analysis to two of four features (sepal length and width) as this allows us to visualize the decision surface of the classifer as a 2d plot.
The data set comprises 150 samples for three species of iris flower, with 50 samples per class.
One class is linearly separable from the others, but measurements for the remaining two overlap.
%
\item\textbf{Wine:} this data set available from the  UCI Machine Learning Repository \cite{Dua_19} consists of chemical analysis data for wines from three different cultivators.
It consists of 178 samples with between 48 and 71 samples per class.
The data is known to be separable \cite{Aeberhard_92}.
%
\item\textbf{Cancer:} this data set available from the UCI Machine Learning Repository \cite{Dua_19} consists of medical analysis data from breast tissue samples.
The 569 samples are classified as either malignant (212 samples) or benign (357 samples).
%
\item\textbf{Digits:} this data set available from the UCI Machine Learning Repository \cite{Dua_19} consists of monochrome images of handwritten digits downsampled to an eight-by-eight grid.
The 1,797 samples are approximately balanced among the 10 digits.
%
\item\textbf{Checker:} for this artificial data set, we sample two features uniformly on the unit square and assign class labels deterministically to create an eight-by-eight checkerboard.
The pattern defeats methods that rely on global properties of the data like correlation, e.g., logistic regression.
It can be recovered successfully by methods that model local structure, e.g., a k-nearest neighbour classifier or decision tree.
The total number of samples is 6,400, so each square of the pattern contains approximately 100 data points.
%
\item\textbf{XOR 6f:} for this artifial data set, we sample six features independently and uniformly on the interval $[-1.0, 1.0]$.
The class label is assigned deterministically based on the sign of the product of features: a positve (or zero) sign is class 1, a negative sign is class 0.
This is similar to the `continuous XOR' problem found in the \texttt{mlbench} library for R \cite{Leisch_21}, except that we only distinguish two classes.
Despite being deterministic, this problem appears to be a hard even for classifiers that model local structure.
The total number of samples is 6,400, so each orthant of the feature space contains approximately 100 data points.
\end{enumerate}
%
The first three experiments in this section consider the impact of $\alpha_v$ and $\alpha_w$ on model behavior.
Proset classifiers are fitted to all six data sets with fixed $M=1,000$ and $\eta=0.5$.
Penalty weights are sampled from the ranges $\lambda_v\in(10^{-6},10^{-1})$ and $\lambda_w\in(10^{-9},10^{-4})$, while the number of batches is allowed to vary between 0 and 10.
The first experiment uses a dominant $l_1$-penalty ($\alpha_v=\alpha_w=0.05$), the second uses balanced penalties ($\alpha_v=\alpha_w=0.50$), and the third a dominant $l_2$-penalty ($\alpha_v=\alpha_w=0.95$).
Results are shown in tables \ref{tab_e1}, \ref{tab_e2}, and \ref{tab_e3}.
A comparison of the first six experiments is found in table \ref{tab_e1_to_e6}.\par
%
The tables describing individual experiments each list the following information:
%
\begin{description}
\item[Data:] the number of classes, as well as the size of the whole data set and train-test split.
%
\item[Candidates:] the approximate number of candidates for prototypes that are actually used.
While $M=1,000$ candidates are specified for each model, the effective maximum for small data sets is 28 \% of the available samples (training data is 70 \% of all samples, five-fold cross-validation leaves 80 \% of training data for model building, $\eta=0.5$ allows at most 50 \% of data in one bin to be used as prototypes).
%
\item[Stage 1:] results for selecting $\lambda_v$ and $\lambda_w$ using a single batch.
Lists the optimal and chosen parameters according to the `one standard error rule' (see algorithm \ref{alg_hyperparameters}), together with the achieved mean log-loss from cross-validation.
The given threshold is the sum of the lowest mean log-loss and the corresponding standard deviation.
%
\item[Stage 2:] results for selecting the $B$ using the penalty weights chosen in stage 1.
%
\item[Final model:] information about the final model fitted on all training data with parameters determined in stages 1 and 2.
The number of features and prototypes, as well as the scores achieved for the test data.
Apart from log-loss, the measures used for evaluation are ROC-AUC and balanced accuracy.\par
%
For multi-class problems, the stated ROC-AUC values is the unweighted average for all pairwise comparisons of two classes.
This generalisation to more than two classes is recommended in \cite{Hand_01} as being robust to class imbalance.\par
%
To compute balanced accuracy, we use the `naive' rule that assigns each sample the class label with the highest predicted probability.
The reported score is the unweighted average of the correct classification rates for each class.
\end{description}
%
\begin{table}
\caption{[E1] Randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.05$)}
\label{tab_e1}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\textbf{Candidates}&$\sim40$&$\sim50$&$\sim160$&$\sim500$&1,000&1,000\\
\multicolumn{7}{|l|}{\textbf{Stage 1}}\\
Optimal $\lambda_v$&$2.5\times10^{-2}$&$6.7\times10^{-6}$&$5.1\times10^{-6}$&$5.6\times10^{-5}$&$1.5\times10^{-3}$&$8.2\times10^{-3}$\\
Selected $\lambda_v$&$5.1\times10^{-2}$&$7.9\times10^{-3}$&$2.9\times10^{-2}$&$1.9\times10^{-3}$&$3.5\times10^{-3}$&$6.0\times10^{-3}$\\
Optimal $\lambda_w$&$3.9\times10^{-5}$&$1.1\times10^{-9}$&$7.0\times10^{-8}$&$2.9\times10^{-9}$&$5.9\times10^{-9}$&$1.4\times10^{-7}$\\
Selected $\lambda_w$&$4.5\times10^{-5}$&$4.6\times10^{-6}$&$7.1\times10^{-5}$&$4.8\times10^{-8}$&$3.9\times10^{-7}$&$2.3\times10^{-6}$\\
Optimal log-loss&0.49&0.11&0.12&0.16&0.17&0.53\\
Threshold&0.65&0.18&0.18&0.19&0.19&0.54\\
Selected log-loss&0.56&0.16&0.14&0.19&0.19&0.54\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal batches&2&2&4&5&1&5\\
Selected batches&2&2&1&1&1&2\\
Optimal log-loss&0.49&0.10&0.13&0.19&0.19&0.53\\
Threshold&0.57&0.18&0.20&0.24&0.20&0.55\\
Selected log-loss&0.49&0.10&0.14&0.20&0.19&0.53\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&1&6&1&18&2&6\\
Prototypes&14&93&22&525&288&415\\
Log-loss&0.62&0.10&0.31&0.15&0.17&0.52\\
ROC-AUC&0.90&1.00&0.94&1.00&0.99&0.82\\
Balanced acc.&0.80&0.96&0.87&0.97&0.95&0.73\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E2] Randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.50$)}
\label{tab_e2}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\textbf{Candidates}&$\sim40$&$\sim50$&$\sim160$&$\sim500$&1,000&1,000\\
\multicolumn{7}{|l|}{\textbf{Stage 1}}\\
Optimal $\lambda_v$&$1.8\times10^{-4}$&$6.7\times10^{-6}$&$6.7\times10^{-3}$&$5.6\times10^{-5}$&$2.6\times10^{-5}$&$8.2\times10^{-3}$\\
Selected $\lambda_v$&$1.0\times10^{-2}$&$3.5\times10^{-2}$&$2.9\times10^{-2}$&$7.3\times10^{-3}$&$3.8\times10^{-4}$&$8.2\times10^{-3}$\\
Optimal $\lambda_w$&$1.1\times10^{-8}$&$1.1\times10^{-9}$&$2.7\times10^{-7}$&$2.9\times10^{-9}$&$3.6\times10^{-9}$&$1.4\times10^{-7}$\\
Selected $\lambda_w$&$6.5\times10^{-6}$&$3.4\times10^{-6}$&$7.1\times10^{-5}$&$8.6\times10^{-9}$&$1.4\times10^{-7}$&$1.4\times10^{-7}$\\
Optimal log-loss&0.52&0.10&0.11&0.17&0.18&0.54\\
Threshold&0.71&0.17&0.15&0.19&0.19&0.54\\
Selected log-loss&0.60&0.16&0.14&0.19&0.19&0.54\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal batches&7&8&9&5&1&1\\
Selected batches&1&1&1&1&1&1\\
Optimal log-loss&0.50&0.17&0.14&0.20&0.19&0.55\\
Threshold&0.58&0.25&0.20&0.23&0.20&0.56\\
Selected log-loss&0.54&0.18&0.14&0.21&0.19&0.55\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&3&2&17&2&6\\
Prototypes&20&17&47&495&382&407\\
Log-loss&0.46&0.12&0.16&0.14&0.18&0.51\\
ROC-AUC&0.91&1.00&0.98&1.00&0.99&0.83\\
Balanced acc.&0.77&0.96&0.91&0.97&0.94&0.74\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E3] Randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.95$)}
\label{tab_e3}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\textbf{Candidates}&$\sim40$&$\sim50$&$\sim160$&$\sim500$&1,000&1,000\\
\multicolumn{7}{|l|}{\textbf{Stage 1}}\\
Optimal $\lambda_v$&$1.8\times10^{-4}$&$6.7\times10^{-6}$&$6.7\times10^{-3}$&$1.3\times10^{-5}$&$2.6\times10^{-5}$&$7.1\times10^{-3}$\\
Selected $\lambda_v$&$1.0\times10^{-2}$&$3.5\times10^{-2}$&$2.9\times10^{-2}$&$7.3\times10^{-3}$&$5.6\times10^{-5}$&$8.2\times10^{-3}$\\
Optimal $\lambda_w$&$1.1\times10^{-8}$&$1.1\times10^{-9}$&$2.7\times10^{-7}$&$2.5\times10^{-9}$&$3.6\times10^{-9}$&$7.0\times10^{-9}$\\
Selected $\lambda_w$&$6.5\times10^{-6}$&$3.4\times10^{-6}$&$7.1\times10^{-5}$&$8.6\times10^{-9}$&$1.7\times10^{-8}$&$1.4\times10^{-7}$\\
Optimal log-loss&0.51&0.10&0.10&0.18&0.18&0.53\\
Threshold&0.69&0.17&0.14&0.21&0.19&0.54\\
Selected log-loss&0.54&0.14&0.13&0.19&0.19&0.54\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal batches&8&3&7&3&4&1\\
Selected batches&1&1&1&1&2&1\\
Optimal log-loss&0.48&0.16&0.14&0.19&0.17&0.55\\
Threshold&0.58&0.24&0.19&0.22&0.18&0.56\\
Selected log-loss&0.52&0.17&0.14&0.20&0.17&0.55\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&4&4&18&2&6\\
Prototypes&23&23&49&533&954&392\\
Log-loss&0.45&0.10&0.16&0.15&0.16&0.52\\
ROC-AUC&0.91&1.00&0.99&1.00&0.99&0.83\\
Balanced acc.&0.79&1.00&0.92&0.97&0.94&0.74\\
\hline
\end{tabular}
\end{center}
\end{table}
%
In the experiments, a dominant $l_1$-penalty give slightly worse results than balanced penalties or a dominant $l_2$-penalty.
In particular, for the iris and cancer data sets, the final models underfit with only one feature each and log-loss worse than the threshold for stage 2.
We believe that this is due to the random selection of candidates for prototypes.
If sample size is small, penalties that work well during cross-validation can apparently be too severe, even though the final model is fitted to the entire training data.
For this reason, we recommend using $\alpha_v=\alpha_w=0.95$ as default values.\par
%
\begin{table}
\caption{[E4] Use optimal hyperparameters ($\alpha_v=\alpha_w=0.95$)}
\label{tab_e4}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\textbf{Candidates}&$\sim40$&$\sim50$&$\sim160$&$\sim500$&1,000&1,000\\
\multicolumn{7}{|l|}{\textbf{Stage 1}}\\
Optimal $\lambda_v$&$1.8\times10^{-4}$&$6.7\times10^{-6}$&$6.7\times10^{-3}$&$1.3\times10^{-5}$&$2.6\times10^{-5}$&$7.1\times10^{-3}$\\
Optimal $\lambda_w$&$1.1\times10^{-8}$&$1.1\times10^{-9}$&$2.7\times10^{-7}$&$2.5\times10^{-9}$&$3.6\times10^{-9}$&$7.0\times10^{-9}$\\
Optimal log-loss&0.51&0.10&0.10&0.18&0.18&0.53\\
Threshold&0.69&0.17&0.14&0.21&0.19&0.54\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal batches&10&10&2&2&8&1\\
Optimal log-loss&0.55&0.17&0.10&0.17&0.17&0.55\\
Threshold&0.64&0.30&0.15&0.19&0.18&0.56\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&13&6&27&2&6\\
Prototypes&265&530&131&952&1,484&423\\
Log-loss&0.45&0.07&0.09&0.14&0.15&0.51\\
ROC-AUC&0.90&1.00&1.00&1.00&0.99&0.83\\
Balanced acc.&0.72&0.96&0.96&0.98&0.94&0.73\\
\hline
\end{tabular}
\end{center}
\end{table}
%
The fourth experiment shows the effect of using the optimal parameters from cross-validation instead of the equivalent sparser solution (see table \ref{tab_e4}).
The models do actually outperform those of the first three experiments slightly.
However, plotting the decision surface of the iris classifier for the third and fourth experiment reveals that using the optimum does still overfit.
The microstructure is too granular and the behavior in regions of the feature space with no data appears random.
We thus recommend using the `one standard error rule', although the model metrics do not suggest that this is absolutely necessary.\par
%
\begin{table}
\caption{[E5] Grid search for $\lambda_v$ ($\lambda_w=10^{-8}$ and $\alpha_v=\alpha_w=0.95$)}
\label{tab_e5}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\textbf{Candidates}&$\sim40$&$\sim50$&$\sim160$&$\sim500$&1,000&1,000\\
\multicolumn{7}{|l|}{\textbf{Stage 1}}\\
Optimal $\lambda_v$&$1.0\times10^{-5}$&$1.0\times10^{-1}$&$1.0\times10^{-2}$&$3.2\times10^{-3}$&$3.2\times10^{-4}$&$1.0\times10^{-2}$\\
Selected $\lambda_v$&$1.0\times10^{-2}$&$1.0\times10^{-1}$&$3.2\times10^{-2}$&$3.2\times10^{-2}$&$3.2\times10^{-4}$&$1.0\times10^{-2}$\\
Optimal log-loss&0.56&0.16&0.10&0.19&0.17&0.52\\
Threshold&0.66&0.25&0.12&0.23&0.19&0.54\\
Selected log-loss&0.58&0.16&0.12&0.22&0.17&0.52\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal batches&2&1&3&1&1&1\\
Selected batches&1&1&1&1&1&1\\
Optimal log-loss&0.50&0.16&0.11&0.22&0.18&0.53\\
Threshold&0.63&0.21&0.14&0.25&0.19&0.54\\
Selected log-loss&0.53&0.16&0.11&0.22&0.18&0.53\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&5&3&18&2&6\\
Prototypes&20&10&45&416&347&364\\
Log-loss&0.45&0.13&0.10&0.19&0.17&0.51\\
ROC-AUC&0.89&1.00&1.00&1.00&0.99&0.84\\
Balanced acc.&0.73&0.96&0.97&0.96&0.94&0.75\\
\hline
\end{tabular}
\end{center}
\end{table}
%
The fifth experiment shows what happens if we fix $\lambda_w$ and perform cross-validation only with respect to $\lambda_v$ and $B$.
Based on the previous experiments, $\lambda_w=10^{-8}$ appears to be a suitable choice.
As stage 1 now deals with a single parameter, we replace the random search with a grid search using 11 points equidistantly spaced on the log-scale.
The results in table \ref{tab_e5} show that fixing the penalty on prototype weights has no meaningful impact on model quality.
Therefore, $\lambda_w=10^{-8}$ is the recommended default for the proset classifier.\par
%
\begin{table}
\caption{[E6] Stage 2 only ($\lambda_v=10^{-3}$, $\lambda_w=10^{-8}$, and $\alpha_v=\alpha_w=0.95$)}
\label{tab_e6}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\textbf{Candidates}&$\sim40$&$\sim50$&$\sim160$&$\sim500$&1,000&1,000\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal batches&9&4&5&8&1&10\\
Selected batches&1&2&1&2&1&5\\
Optimal log-loss&0.51&0.14&0.10&0.15&0.20&0.51\\
Threshold&0.61&0.19&0.17&0.18&0.21&0.52\\
Selected log-loss&0.56&0.15&0.13&0.16&0.20&0.52\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&9&6&32&2&6\\
Prototypes&32&73&106&829&247&1,473\\
Log-loss&0.69&0.07&0.10&0.14&0.19&0.48\\
ROC-AUC&0.85&1.00&0.99&1.00&0.99&0.85\\
Balanced acc.&0.64&0.98&0.97&0.97&0.95&0.75\\
\hline
\end{tabular}
\end{center}
\end{table}
%
The sixth experiment considers fixing both $\lambda_w$ and $\lambda_v$ (see table \ref{tab_e6}).
The former is again set to $\lambda_w=10^{-8}$, the latter to $\lambda_w=10^{-3}$, which is close to the parameters selected in previous experiments.
Here, we observe a worsening of the score for the iris data set, but the remaining scores are comparable to or even better than those from the full parameter search.
Based on this finding, we recommend $\lambda_w=10^{-3}$ as default for the classifier.\par
%
\begin{table}
\caption{[E1--E6] Comparison of results (best log-loss bold)}
\label{tab_e1_to_e6}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{[E1] Randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.05$)}}\\
Active features&1&6&1&18&2&6\\
Prototypes&14&93&22&525&288&415\\
Log-loss&0.62&0.10&0.31&0.15&0.17&0.52\\
Threshold stage 2&0.57&0.18&0.20&0.24&0.20&0.55\\
ROC-AUC&0.90&1.00&0.94&1.00&0.99&0.82\\
Balanced acc.&0.80&0.96&0.87&0.97&0.95&0.73\\
\multicolumn{7}{|l|}{\textbf{[E2] Randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.50$)}}\\
Active features&2&3&2&17&2&6\\
Prototypes&20&17&47&495&382&407\\
Log-loss&0.46&0.12&0.16&\textbf{0.14}&0.18&0.51\\
Threshold stage 2&0.58&0.25&0.20&0.23&0.20&0.56\\
ROC-AUC&0.91&1.00&0.98&1.00&0.99&0.83\\
Balanced acc.&0.77&0.96&0.91&0.97&0.94&0.74\\
\multicolumn{7}{|l|}{\textbf{[E3] Randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.95$)}}\\
Active features&2&4&4&18&2&6\\
Prototypes&23&23&49&533&954&392\\
Log-loss&\textbf{0.45}&0.10&0.16&0.15&0.16&0.52\\
Threshold stage 2&0.58&0.24&0.19&0.22&0.18&0.56\\
ROC-AUC&0.91&1.00&0.99&1.00&0.99&0.83\\
Balanced acc.&0.79&1.00&0.92&0.97&0.94&0.74\\
\multicolumn{7}{|l|}{\textbf{[E4] Use optimal hyperparameters ($\alpha_v=\alpha_w=0.95$)}}\\
Active features&2&13&6&27&2&6\\
Prototypes&265&530&131&952&1,484&423\\
Log-loss&\textbf{0.45}&\textbf{0.07}&\textbf{0.09}&\textbf{0.14}&\textbf{0.15}&0.51\\
Threshold stage 2&0.64&0.30&0.15&0.19&0.18&0.56\\
ROC-AUC&0.90&1.00&1.00&1.00&0.99&0.83\\
Balanced acc.&0.72&0.96&0.96&0.98&0.94&0.73\\
\multicolumn{7}{|l|}{\textbf{[E5] Grid search for $\lambda_v$ ($\lambda_w=10^{-8}$ and $\alpha_v=\alpha_w=0.95$)}}\\
Active features&2&5&3&18&2&6\\
Prototypes&20&10&45&416&347&364\\
Log-loss&\textbf{0.45}&0.13&0.10&0.19&0.17&0.51\\
Threshold stage 2&0.63&0.21&0.14&0.25&0.19&0.54\\
ROC-AUC&0.89&1.00&1.00&1.00&0.99&0.84\\
Balanced acc.&0.73&0.96&0.97&0.96&0.94&0.75\\
\multicolumn{7}{|l|}{\textbf{[E6] Stage 2 only  ($\lambda_v=10^{-3}$, $\lambda_w=10^{-8}$, and $\alpha_v=\alpha_w=0.95$)}}\\
Active features&2&9&6&32&2&6\\
Prototypes&32&73&106&829&247&1,473\\
Log-loss&0.69&\textbf{0.07}&0.10&\textbf{0.14}&0.19&\textbf{0.48}\\
Threshold stage 2&0.61&0.19&0.17&0.18&0.21&0.52\\
ROC-AUC&0.85&1.00&0.99&1.00&0.99&0.85\\
Balanced acc.&0.64&0.98&0.97&0.97&0.95&0.75\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E7] Stage 2, vary candidates ($\lambda_v=10^{-3}$, $\lambda_w=10^{-8}$, and $\alpha_v=\alpha_w=0.95$)}
\label{tab_e7}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Checker}&\textbf{Checker}&\textbf{Checker}&\textbf{XOR 6f}&\textbf{XOR 6f}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&2&2&2&2&2&2\\
Features&2&2&2&6&6&6\\
Samples&6,400&6,400&6,400&6,400&6,400&6,400\\
Train samples&4,480&4,480&4,480&4,480&4,480&4,480\\
Test samples&1,920&1,920&1,920&1,920&1,920&1,920\\
\textbf{Candidates}&100&300&1,500&100&300&1,500\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal batches&6&8&1&10&8&10\\
Selected batches&2&1&1&7&4&7\\
Optimal log-loss&0.42&0.29&0.20&0.58&0.55&0.51\\
Threshold&0.44&0.31&0.20&0.59&0.56&0.51\\
Selected log-loss&0.43&0.29&0.20&0.59&0.56&0.51\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&2&2&6&6&6\\
Prototypes&117&140&316&389&593&2,884\\
Log-loss&0.39&0.30&0.18&0.57&0.53&0.48\\
ROC-AUC&0.94&0.96&0.99&0.77&0.81&0.85\\
Balanced acc.&0.86&0.88&0.95&0.69&0.73&0.76\\
\hline
\end{tabular}
\end{center}
\end{table}
%
The final experiment shows the impact of different batch sizes $M$ (see table \ref{tab_e7}).
It deals only with the checkerboard and XOR data, as the maximum batch size for the first four data sets is limited by their small sample size.
For each of the two large data sets, we try using 100, 300, and 1,500 samples instead of the default $M=1,000$.
The model metrics for 100 and 300 samples are worse than before, while the results for 1,500 candidates are comparable.
Clearly, there is an advantage to using large batches as long as the sample size permits.
%
\clearpage
%
\subsection{Comparison to other classifiers}
\label{sec_comparison_classifier}
%
In this section, we compare the proset classifier to the $k$-nearest neighbor (KNN) and XGBoost methods.
The former may be the most similar to proset conceptually, as it scores new points based on their proximity to training samples.
In this sense, both are `bottom up' approaches to classification.
However, there are at least three major differences between proset and KNN:
%
\begin{enumerate}
\item KNN uses all training samples for scoring and gives equal importance to each sample.
Proset selects prototypes and assigns individual weights.
%
\item KNN relies on the user's choice of features and distance metric.
It always uses all provided features, which means that too many irrelevant variables can degrade performance.
Proset adapts the metric to the problem and removes features that contribute little.
%
\item KNN has no notion of absolute distance.
The nearest neighbors have the same impact on classification no matter how far away they are from the sample being scored.
For proset, remote prototypes have negligible impact on classification.
As the distance of a sample to the whole prototype set increases, the predicted probabilities converge to the marginals of the training data.
\end{enumerate}
%
XGBoost is a particular implementation of gradient boosting for decision trees (we do not consider other base learners in this study) \cite{Chen_16}.
It has become a kind of industry standard for supervised learning outside the domain of deep learning.
Unlike proset and KNN, decision tree ensembles like XGBoost follow a `top down' strategy.
Despite this fundamental difference, we try to relate it to proset and KNN in terms of the three points stated above:
%
\begin{enumerate}
\item XGBoost does not retain any training samples.
It generates a set of rules in the form of decision trees and performs classification via weighted voting.
%
\item XGBoost relies only on the ordering of features, not on any kind of distance metric.
When building trees, the algorithm selects features using a greedy heuristic.
It is considered good practice to limit the number of features evaluated at each stage by subsampling \cite{Chen_16}.
This results in a more diverse set of trees with less tendency to overfit.
Even with this approach, features that have negligible impact on performance are unlikely to be selected.
%
\item Using XGBoost to score a sample that is far away from the training data gives arbitrary results.
The decision trees have not been validated in that part of the feature space.
\end{enumerate}
%
To fit KNN and XGBoost models, we follow the same general strategy as for proset.
We determine optimal hyperparameters using five-fold cross-validation and then choose an equivalent set of parameters that is less likely to overfit via the `one standard error rule' (see algorithm \ref{alg_hyperparameters}).
This is very simple to impement for KNN, since the sole hyperparameter is the number of neighbors $k$.
Note that we choose the \textit{largest} $k$ from the set of equivalent solutions to achieve a high degree of smoothing.\par
%
For XGBoost, tuning is more complex as the number of parameters that can be optimized is quite large.
Based on prior experience, we focus on the following five only, leaving all others at their recommended defaults:
%
\begin{enumerate}
\item Learning rate $\eta$.
Controls the impact of each additional tree on the prediction.
Choosing a smaller $\eta$ can increase model performance slightly but requires more boosting iterations.
%
\item Number of boosting iterations.
Has to be sufficiently large for the model to capture all of the structure in the data.
Increasing it further leads to saturation instead of overfitting, i.e., the model quality on test data fluctuates around a common level.
%
\item Maximum tree depth.
Controls the complexity of each tree.
Our experience is that XGBoost performs best if the individual trees underfit.
%
\item Fraction of features evaluated per split.
Using a random subset of the features in each split results in a more diverse set of trees.
%
\item Fraction of records used for training each tree.
Using a random subset of the training samples for each tree also results in a more diverse set of trees.
\end{enumerate}
%
We follow a similar strategy as for proset (see algorithm \ref{alg_hyperparameters}) to fit XGBoost classifiers.
In the first stage, we fix $\eta=0.1$ and use 100 boosting iterations to determine suitable values for maximum tree depth, fraction of features per split, and fraction of records per tree.
Hyperparameter values are sampled randomly to generated 100 trial combinations:
%
\begin{itemize}
\item Maximum tree depth is sampled from 0 to 9 with equal probability.
Note that 0 already corresponds to a single split, so the trees have between 2 and 1024 leaf nodes.
In case model performance was inadequate, we increased the maximum parameter value to 19.
%
\item Fraction of features per split is sampled uniformly between 0.1 and 0.9 if the model has more than two features.
For two features, we just leave the parameter at 1.0.
%
\item Fraction of records per tree is sampled uniformly between 0.1 and 0.9.
\end{itemize}
%
After five-fold cross-validation, we determine the parameter combination that minimizes log-loss, compute a threshold, and choose an equivalent combination.
Among all candidates, we use the one that minimizes the depth of the tree first, then the fraction of features per split, and finally the fraction of records per tree.\par
%
In the second stage, we fix $\eta=0.01$ and use five-fold cross-validation to determine the number of boosting iterations up to a maximum of 1,000.
The optimum is usually at the upper bound but we again apply the `one standard error rule' to find an equivalent smaller number.
The model is then re-fitted to all training data and scored on test data.\par
%
\begin{table}
\caption{[E8] $k$-nearest neighbor classifier with grid search for $k$}
\label{tab_e8}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\multicolumn{7}{|l|}{\textbf{Cross-validation}}\\
Optimal $k$&14&6&30&8&8&10\\
Selected $k$&23&11&54&43&12&13\\
Optimal log-loss&0.45&0.11&0.12&0.16&0.22&0.53\\
Threshold&0.49&0.13&0.16&0.27&0.25&0.53\\
Selected log-loss&0.49&0.13&0.16&0.27&0.25&0.53\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Log-loss&0.43&0.09&0.18&0.22&0.22&0.52\\
ROC-AUC&0.92&1.00&0.98&1.00&0.98&0.83\\
Balanced acc.&0.78&1.00&0.90&0.93&0.91&0.74\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E9] XGBoost classifier with randomized parameter search}
\label{tab_e9}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{Data}}\\
Classes&3&3&2&10&2&2\\
Features&2&13&30&64&2&6\\
Samples&150&178&569&1,797&6,400&6,400\\
Train samples&105&124&398&1,257&4,480&4,480\\
Test samples&45&54&171&540&1,920&1,920\\
\multicolumn{7}{|l|}{\textbf{Stage 1}}\\
Optimal max.\ depth&1&7&8&4&9&0\\
Selected max.\ depth&1&1&1&3&9&0\\
Optimal colsample&n/a&0.28&0.83&0.27&n/a&0.50\\
Selected colsample&n/a&0.27&0.31&0.24&n/a&0.30\\
Optimal subsample&0.58&0.81&0.35&0.77&0.83&0.82\\
Selected subsample&0.22&0.86&0.40&0.72&0.83&0.26\\
Optimal log-loss&0.52&0.10&0.09&0.13&0.07&0.69\\
Threshold&0.55&0.15&0.10&0.15&0.08&0.69\\
Selected log-loss&0.54&0.12&0.10&0.15&0.07&0.69\\
\multicolumn{7}{|l|}{\textbf{Stage 2}}\\
Optimal iterations&1,000&1,000&1,000&1,000&1,000&850\\
Selected iterations&331&575&281&767&927&138\\
Optimal log-loss&0.54&0.11&0.11&0.15&0.08&0.69\\
Threshold&0.62&0.17&0.17&0.18&0.09&0.69\\
Selected log-loss&0.62&0.17&0.17&0.18&0.09&0.69\\
\multicolumn{7}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&13&18&53&2&0\\
Log-loss&0.58&0.16&0.21&0.15&0.06&0.69\\
ROC-AUC&0.91&1.00&0.98&1.00&1.00&0.50\\
Balanced acc.&0.78&1.00&0.91&0.97&0.99&0.50\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E3, E8, E9] Comparison of results (best log-loss bold)}
\label{tab_e3_e8_e9}
%
\begin{center}
\small
\begin{tabular}{|lrrrrrr|}
\hline
&\multicolumn{6}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Iris 2f}&\textbf{Wine}&\textbf{Cancer}&\textbf{Digits}&\textbf{Checker}&\textbf{XOR 6f}\\
\multicolumn{7}{|l|}{\textbf{[E3] Proset with randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.95$)}}\\
Active features&2&4&4&18&2&6\\
Log-loss&0.45&0.10&\textbf{0.16}&\textbf{0.15}&0.16&\textbf{0.52}\\
Threshold stage 2&0.58&0.24&0.19&0.22&0.18&0.56\\
ROC-AUC&0.91&1.00&0.99&1.00&0.99&0.83\\
Balanced acc.&0.79&1.00&0.92&0.97&0.94&0.74\\
\multicolumn{7}{|l|}{\textbf{[E8] $k$-nearest neighbor classifier with grid search for $k$}}\\
Active features&2&13&30&64&2&6\\
Log-loss&\textbf{0.43}&\textbf{0.09}&0.18&0.22&0.22&\textbf{0.52}\\
Threshold CV&0.49&0.13&0.16&0.27&0.25&0.54\\
ROC-AUC&0.92&1.00&0.98&1.00&0.98&0.83\\
Balanced acc.&0.78&1.00&0.90&0.93&0.91&0.74\\
\multicolumn{7}{|l|}{\textbf{[E9] XGBoost classifier with randomized parameter search}}\\
Active features&2&13&18&53&2&0\\
Log-loss&0.58&0.16&0.21&\textbf{0.15}&\textbf{0.06}&0.69\\
Threshold stage 2&0.62&0.17&0.17&0.18&0.09&0.69\\
ROC-AUC&0.91&1.00&0.98&1.00&1.00&0.50\\
Balanced acc.&0.78&1.00&0.91&0.97&0.99&0.50\\
\hline
\end{tabular}
\end{center}
\end{table}
%
Results for fitting KNN and XGBoost classifiers to the six examples used for the proset parameter study are given in tables \ref{tab_e8} and \ref{tab_e9}.
The structure of the tables and reported metrics are essentially the same as for proset in the previous section.
For XGBoost, the number of active features reported is the number of features having positive importance using the method's built-in importance scores.\par
%
Table \ref{tab_e3_e8_e9} summarizes the performance of all three models.
For proset, reference values are taken from the full parameters search using $\alpha_v=\alpha_w=0.95$ (see table \ref{tab_e3}).
Somewhat surprisingly, there is considerable variation in model performance for the test cases:
%
\begin{enumerate}
\item\textbf{Iris 2f:} the KNN model is best in terms of log-loss, but the log-loss for proset is still below the equivalence threshold for KNN.
The log-loss for XGBoost is worse than the threshold, although ROC-AUC and balanced accuracy are comparable.
%
\item\textbf{Wine:} as for iris data.
%
\item\textbf{Cancer:} proset is best, with the log-loss for KNN still below the equivalence threshold.
As before, XGBoost has worse log-loss but similar performance in terms of ROC-AUC and balanced accuracy.
%
\item\textbf{Digits}: proset and XGBoost have identical performance, except that XGBoost has a tighter threshold for equivalence.
KNN performs worse, with log-loss above the threshold for XGBoost and balanced accuracy lower by four percentage points.
%
\item\textbf{Checker:} here, we get a strict ranking in terms of equivalence.
XGBoost outperforms proset, which in turn outperforms KNN.
%
\item\textbf{XOR 6f:} KNN and proset have identical performance, while XGBoost is unable to find any structure.
\end{enumerate}
%
We draw the following conclusions from this study:
%
\begin{itemize}
\item It is hard to improve on KNN if the training data has few features, i.e., if there are few or no irrelevant variables.
For small sample size, the cross-validation variance is also lower than for proset and XGBoost.
The reason is probably that KNN only has a single hyperparameter and does not rely on any kind of random subsampling or partitioning.
Of the three algorithms, proset shows the largest variance, although there appears to be no comparable impact on the performance of the final model.
%
\item The extremely good performance of XGBoost on the checker data is likely due to the fact that the approximating function is of the same class as the target.
Both are step functions with edges parallel to the main coordinate axes.
%
\item XOR 6f defeats tree ensembles, which choose features one at a time based on a greedy criterion.
However, for a single split, the expected number of cases per class in each resulting subspace is always 50 \%, same as for the whole space.
Thus, the first split can only model random fluctuations in the training data.
While the same is true for the checkerboard data with an even number of squares per side, splitting this data randomly is almost sure to break the symmetry.
In contrast, for XOR 6f, at least five splits are required to expose structure.
But the partitions created by five random splits apparenty do not contain sufficient information to build a meaningful decision tree.
\end{itemize}
%
Based on these observations, we define five additional test cases to further explore the differences between the three algorithms:
%
\begin{enumerate}
\item\textbf{Checker rot:} to determine how much of the good performance of XGBoost on the checker data is due to the similarity between the approximating function and the target, we rotate the checkerboard pattern by 45\textdegree.
%
\item\textbf{XOR 3f, XOR 4f, XOR 5f:} in order to find the point at which XGBoost fails on the `continuous XOR' problem, we generate instances with three, four, and five features.
As for XOR 6f, these each have an average of 100 samples per orthant.
%
\item\textbf{XOR 6+6f:} to understand how easy it is to confuse the KNN classifier with irrelevant data, we add six more features to the XOR 6f problem.
These are drawn from the same distribution as the first six but have no impact on the target.
\end{enumerate}
%
\begin{table}
\caption{[E10] New examples -- results for proset classifier}
\label{tab_e10}
%
\begin{center}
\small
\begin{tabular}{|lrrrrr|}
\hline
&\multicolumn{5}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Checker rot}&\textbf{XOR 3f}&\textbf{XOR 4f}&\textbf{XOR 5f}&\textbf{XOR 6+6f}\\
\multicolumn{6}{|l|}{\textbf{Data}}\\
Classes&2&2&2&2&2\\
Features&2&3&4&5&12\\
Samples&6,400&800&1,600&3,200&6,400\\
Train samples&4,480&560&1,120&2,240&4,480\\
Test samples&1,920&240&480&960&1,920\\
\textbf{Candidates}&1,000&$\sim220$&$\sim450$&$\sim900$&1,000\\
\multicolumn{6}{|l|}{\textbf{Stage 1}}\\
Optimal $\lambda_v$&$4.9\times10^{-5}$&$6.7\times10^{-3}$&$2.4\times10^{-2}$&$9.5\times10^{-3}$&$3.5\times10^{-4}$\\
Selected $\lambda_v$&$3.8\times10^{-4}$&$1.2\times10^{-2}$&$2.0\times10^{-2}$&$8.1\times10^{-3}$&$8.1\times10^{-4}$\\
Optimal $\lambda_w$&$4.7\times10^{-9}$&$2.7\times10^{-9}$&$3.6\times10^{-9}$&$2.1\times10^{-9}$&$6.2\times10^{-7}$\\
Selected $\lambda_w$&$1.4\times10^{-7}$&$3.0\times10^{-7}$&$2.7\times10^{-7}$&$1.3\times10^{-7}$&$9.3\times10^{-7}$\\
Optimal log-loss&0.18&0.18&0.28&0.40&0.55\\
Threshold&0.20&0.22&0.32&0.41&0.56\\
Selected log-loss&0.20&0.19&0.31&0.41&0.56\\
\multicolumn{6}{|l|}{\textbf{Stage 2}}\\
Optimal batches&1&1&1&5&1\\
Selected batches&1&1&1&2&1\\
Optimal log-loss&0.20&0.20&0.31&0.40&0.57\\
Threshold&0.21&0.23&0.35&0.42&0.58\\
Selected log-loss&0.20&0.20&0.31&0.42&0.57\\
\multicolumn{6}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&3&4&5&6\\
Prototypes&340&63&85&307&456\\
Log-loss&0.19&0.16&0.33&0.40&0.54\\
ROC-AUC&0.99&0.99&0.96&0.91&0.81\\
Balanced acc.&0.95&0.96&0.87&0.82&0.71\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E11]  New examples -- results for $k$-nearest neighbor classifier}
\label{tab_e11}
%
\begin{center}
\small
\begin{tabular}{|lrrrrr|}
\hline
&\multicolumn{5}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Checker rot}&\textbf{XOR 3f}&\textbf{XOR 4f}&\textbf{XOR 5f}&\textbf{XOR 6+6f}\\
\multicolumn{6}{|l|}{\textbf{Data}}\\
Classes&2&2&2&2&2\\
Features&2&3&4&5&12\\
Samples&6,400&800&1,600&3,200&6,400\\
Train samples&4,480&560&1,120&2,240&4,480\\
Test samples&1,920&240&480&960&1,920\\
\multicolumn{6}{|l|}{\textbf{Cross-validation}}\\
Optimal $k$&8&7&7&12&95\\
Selected $k$&12&12&10&14&100\\
Optimal log-loss&0.22&0.23&0.31&0.44&0.70\\
Threshold&0.25&0.26&0.34&0.45&0.70\\
Selected log-loss&0.25&0.26&0.34&0.45&0.70\\
\multicolumn{6}{|l|}{\textbf{Final model, scores for test data}}\\
Log-loss&0.22&0.21&0.35&0.45&0.70\\
ROC-AUC&0.98&0.99&0.93&0.89&0.47\\
Balanced acc.&0.91&0.94&0.85&0.79&0.48\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E12]  New examples -- results for XGBoost classifier}
\label{tab_e12}
%
\begin{center}
\small
\begin{tabular}{|lrrrrr|}
\hline
&\multicolumn{5}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Checker rot}&\textbf{XOR 3f}&\textbf{XOR 4f}&\textbf{XOR 5f}&\textbf{XOR 6+6f}\\
\multicolumn{6}{|l|}{\textbf{Data}}\\
Classes&2&2&2&2&2\\
Features&2&3&4&5&12\\
Samples&6,400&800&1,600&3,200&6,400\\
Train samples&4,480&560&1,120&2,240&4,480\\
Test samples&1,920&240&480&960&1,920\\
\multicolumn{6}{|l|}{\textbf{Stage 1}}\\
Optimal max.\ depth&9&7&9&16&0\\
Selected max.\ depth&8&5&8&0&0\\
Optimal colsample&n/a&0.81&0.73&0.84&0.86\\
Selected colsample&n/a&0.90&0.66&0.30&0.50\\
Optimal subsample&0.40&0.84&0.65&0.79&0.68\\
Selected subsample&0.34&0.73&0.75&0.26&0.82\\
Optimal log-loss&0.19&0.12&0.43&0.67&0.69\\
Threshold&0.20&0.15&0.45&0.72&0.69\\
Selected log-loss&0.20&0.14&0.44&0.69&0.69\\
\multicolumn{6}{|l|}{\textbf{Stage 2}}\\
Optimal iterations&1,000&1,000&1,000&762&812\\
Selected iterations&956&854&877&756&41\\
Optimal log-loss&0.19&0.14&0.47&0.69&0.69\\
Threshold&0.20&0.16&0.49&0.69&0.69\\
Selected log-loss&0.20&0.16&0.49&0.69&0.69\\
\multicolumn{6}{|l|}{\textbf{Final model, scores for test data}}\\
Active features&2&3&4&0&0\\
Log-loss&0.18&0.13&0.46&0.70&0.69\\
ROC-AUC&0.99&1.00&0.89&0.50&0.50\\
Balanced acc.&0.95&0.96&0.81&0.50&0.50\\
\hline
\end{tabular}
\end{center}
\end{table}
%
\begin{table}
\caption{[E10, E11, E12] Comparison of results (best log-loss bold)}
\label{tab_e10_e11_e12}
%
\begin{center}
\small
\begin{tabular}{|lrrrrr|}
\hline
&\multicolumn{5}{c|}{\textbf{\hrulefill\ Dataset \hrulefill}}\\
&\textbf{Checker rot}&\textbf{XOR 3f}&\textbf{XOR 4f}&\textbf{XOR 5f}&\textbf{XOR 6+6f}\\
\multicolumn{6}{|l|}{\textbf{[E10] Proset with randomized search for $\lambda_v$ and $\lambda_w$ ($\alpha_v=\alpha_w=0.95$)}}\\
Active features&2&3&4&5&6\\
Log-loss&0.19&0.16&\textbf{0.33}&\textbf{0.40}&\textbf{0.54}\\
Threshold stage 2&0.21&0.23&0.35&0.42&0.58\\
ROC-AUC&0.99&0.99&0.96&0.91&0.81\\
Balanced acc.&0.95&0.96&0.87&0.82&0.71\\
\multicolumn{6}{|l|}{\textbf{[E11] $k$-nearest neighbor classifier with grid search for $k$}}\\
Active features&2&3&4&5&12\\
Log-loss&0.22&0.21&0.35&0.45&0.70\\
Threshold CV&0.25&0.26&0.34&0.45&0.70\\
ROC-AUC&0.98&0.99&0.93&0.89&0.47\\
Balanced acc.&0.91&0.94&0.85&0.79&0.48\\
\multicolumn{6}{|l|}{\textbf{[E12] XGBoost classifier with randomized parameter search}}\\
Active features&2&3&4&n/a&n/a\\
Log-loss&\textbf{0.18}&\textbf{0.13}&0.46&0.70&0.69\\
Threshold stage 2&0.20&0.16&0.49&0.69&0.69\\
ROC-AUC&0.99&1.00&0.89&0.50&0.50\\
Balanced acc.&0.95&0.96&0.81&0.50&0.50\\
\hline
\end{tabular}
\end{center}
\end{table}
%
Results for proset, KNN, and XGBoost are summarized in tables \ref{tab_e10}, \ref{tab_e11}, and \ref{tab_e12}.
A comparison of the three algorithms is shown in table \ref{tab_e10_e11_e12}:
%
\begin{enumerate}
\item\textbf{Checker rot:} XGBoost still yields the best model on this pattern, but the log-loss for proset is now below the threshold for equivalence.
The metrics for both are comparable to those for proset on the original checkerboard.
Results for KNN are slightly worse.
%
\item\textbf{XOR 3f, XOR 4f, XOR 5f:} with three features, XGBoost performs best, proset is barely equivalent, and KNN worse.
XGBoost likely has some advantage here because the target is again a step function with axis-parallel edges.
With four features, proset is best, KNN is barely equivalent, and XGBoost worse.
With five features, proset outperforms KNN, while XGBoost is unable to find any structure.
%
\item\textbf{XOR 6+6f:} proset correctly selects the six relevant features.
The metrics for the resulting model are slightly worse than before but log-loss is still below the threshold of equivalence for XOR 6f.
Neither KNN nor XGBoost are able to find any structure.
For KNN, the chosen $k$ is very large, so the model approximates the marginal distribution.
Apparently, any local structure that emerges for smaller values of $k$ is entirely spurious.
\end{enumerate}
%
The model comparison shows that proset is a worthwhile addition to the supervised learning toolbox.
It behaves similar to KNN if the number of features is small and outperforms it in the presence of irrelevant features.
It also appears to be better than tree ensembles at identifying higher order dependencies between the features and target.
In all cases where reducing the number of features is feasible (wine, cancer, digits), proset selects the smallest subset with no appreciable loss in model quality.
\endinput
